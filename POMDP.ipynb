{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planificación en entornos con incertidumbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introducción\n",
    "\n",
    "Planificar consiste en encontrar una secuencia de acciones para alcanzar un\n",
    "determinado objetivo cuando se ejecutan a partir de un determinado estado inicial.\n",
    "Existen multitud de aplicaciones en el mundo real y, en particular, veremos en este\n",
    "trabajo algunas relacionadas con la Robótica.\n",
    "Comencemos definiendo algunos conceptos en un problema de planificación:\n",
    "\n",
    "* Un estado s ∈ S es una posible situación del problema de acuerdo a una deter-\n",
    "minada representación, siendo S el conjunto de todas las posibles situaciones en\n",
    "dicha representación.\n",
    "\n",
    "* Una acción a : S → S es un operador básico para la transformación de estados.\n",
    "Denotaremos por A al conjunto de las acciones definidas para resolver el problema.\n",
    "Cabe mencionar que cada acción a ∈ A tiene asociada una función de aplicabilidad\n",
    "$f_a$ : S → {0, 1} que determina si la acción puede ser aplicable o no a un estado\n",
    "concreto.\n",
    "De esta forma, un plan es una secuencia de acciones que, partiendo de un estado inicial\n",
    "$s_{init}$ ∈ $S$, permite alcanzar un estado final $s_{goal}$ ∈ $S_{goal}$ al que llamaremos objetivo, siendo\n",
    "$S_{goal}$ ⊆ $S$ el conjunto de aquellos estados considerados como finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Planificación clásica\n",
    "La planificación clásica considera entornos que son:\n",
    "Completamente observables: El planificador conoce siempre el estado actual.\n",
    "Deterministas: El efecto de cada acción sobre cada estado es conocido y siempre\n",
    "es el mismo.\n",
    "* Finitos: Los conjuntos de estados y acciones son finitos.\n",
    "* Estáticos: La situación del problema sólo puede cambiar mediante la las acciones del planificador.\n",
    "* Discretos: La situación del problema se puede describir de forma discreta:\n",
    "    * Tiempo discreto\n",
    "    * Acciones discretas\n",
    "    * Objetos discretos\n",
    "    * Efectos discretos\n",
    "\n",
    "Por desgracia, muchos problemas de planificación interesantes de la vida real no pueden\n",
    "ser resueltos mediante planificación clásica, fallando en algunas o en todas las anteriores asunciones. Tomemos por ejemplo, el caso de un robot móvil de dos ruedas cuya\n",
    "misión es encontrar la puerta del despacho de un determinado profesor en el pasillo del\n",
    "Departamento de CCIA en la primera planta del módulo H de la ETSII. El robot tiene\n",
    "tres acciones: (1) Avanzar a velocidad constante, (2) detenerse y (3) leer un nombre en\n",
    "una puerta. Los sensores del robot son capaces de detectar puertas, capturar imágenes y\n",
    "saber cuantas vueltas ha dado cada una de sus ruedas. El robot cuenta con un software\n",
    "OCR para interpretar los nombres escritos en las imágenes capturadas. Cuando encuen-\n",
    "tre la puerta del profesor que busca, debe detenerse y mostrar en una pantalla el número\n",
    "de metros recorridos.\n",
    "* El entorno no es $completamente\\ observable$: Pues la percepción del robot depende\n",
    "de la calidad de los sensores y del procesamiento realizado con la información\n",
    "obtenida. Podrı́an haber problemas de calidad en las imágenes capturadas o errores\n",
    "en su procesamiento.\n",
    "* Las acciones no son $deterministas$: Pues las condiciones del suelo y la calidad de\n",
    "los motores afectarı́an a las trayectorias realizadas en mayor o menor medida.\n",
    "Asimismo, podrı́a existir un bias en los motores que no estuviera recogido en el\n",
    "modelo fı́sico del robot.\n",
    "\n",
    "Este trabajo se va a enfocar en la resolución de problemas de planificación en entornos\n",
    "$parcialmente\\ observables,\\ estocásticos,\\ finitos,\\ dinámicos\\ y\\ discretos$. Para ello, se van a\n",
    "utilizar dos formas nuevas de definir los problemas de planificación: Mediante $Procesos\\ de\\ Decisión\\ de\\ Markov\\ (MDPs)$ y mediante $Procesos\\ de\\ Decisión\\ de\\ Markov\\ Parcialmente\\ Observables\\ (POMDPs)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Procesos de Decisión de Markov\n",
    "Un Proceso de Decisión de Markov (MDP) se define como una tupla (S, A, T, R),\n",
    "en dónde:\n",
    "* $S$ es un conjunto finito de estados.\n",
    "* $A$ es un conjunto finito de posibles acciones.\n",
    "* $T\\ :\\ S\\ ×\\ A\\ ×\\ S$ → [0, 1] es la función de transición, dónde $T(s, a, s')$ = $Pr(s'|s, a)$\n",
    "representa la probabilidad de obtener el estado $s'$ cuando se realiza la acción a\n",
    "partiendo del estado $s$.\n",
    "* $R\\ :\\ S\\ ×\\ A\\ ×\\ S$ → $R$ es la función de recompensa, dónde $R(s, a, s')$ es la recompensa\n",
    "obtenida tras ejecutar la acción a partiendo del estado $s$ y alcanzando $s'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los $MDPs$ no se define de forma explı́cita un objetivo o conjunto de estados finales,\n",
    "en su lugar se define una función de recompensa. Resolver un $MDP$ significa encontrar\n",
    "un criterio o polı́tica para elegir acciones de tal forma que se maximice el lı́mite del\n",
    "sumatorio de la función de recompensa en el infinito.\n",
    "A modo de ejemplo, en la figura 1 se observa el diagrama de un $MDP$ con tres estados\n",
    "$S_0$ , $S_1$ , $S_2$ y dos acciones $a_0$ , $a_1$. Las probabilidades de la función de transición quedan\n",
    "representadas sobre las aristas del grafo, por ejemplo, la probabilidad de alcanzar el\n",
    "estado $S_0$ tras ejecutar la acción a 0 partiendo de $S_1$ es 0,7. La función de recompensa\n",
    "devuelve siempre cero salvo cuando se alcanza $S_0$ desde $S_1$ aplicando a 0 que devuelve un\n",
    "valor de 5 y cuando se alcanza $S_1$ aplicando a 1 desde $S_2$ , devolviendo un valor de −1.\n",
    "Una posible polı́tica para el anterior $MDP$ podrı́a ser la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ejecutar siempre $a_1$ desde el estado $S_0$\n",
    "* Ejecutar siempre $a_0$ desde el estado $S_1$\n",
    "* Ejecutar siempre $a_1$ desde el estado $S_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Procesos de Decisión de Markov Parcialmente Observables\n",
    "Un Proceso de Decisión de Markov Parcialmente Observable (POMDP) es\n",
    "una tupla (S, A, T, R, Z, O), en dónde:\n",
    "* $S$ es un conjunto finito de estados.\n",
    "\n",
    "* $A$ es un conjunto finito de posibles acciones.\n",
    "\n",
    "* $T\\ :\\ S\\ ×\\ A\\ ×\\ S$ → [0, 1] es la función de transición, dónde $T(s, a, s')$ = $Pr(s'|s, a)$\n",
    "representa la probabilidad de obtener el estado $s'$ cuando se realiza la acción a\n",
    "partiendo del estado $s$.\n",
    "\n",
    "* $R\\ :\\ S\\ ×\\ A$ → $R$ es la función de recompensa, dónde $R(s, a)$ es la recompensa\n",
    "obtenida tras ejecutar la acción a partiendo del estado $s$.\n",
    "\n",
    "* $Z$ es un conjunto finito de posibles observaciones.\n",
    "\n",
    "* $O\\ :\\ S\\ ×\\ A\\ ×\\ Z$ → [0, 1] es la función de observación, dónde $O(s', a, z)$ = $Pr(z|a, s')$\n",
    "es la probabilidad de observar $z$ cuando se ejecuta la acción $a$ y el estado resultante\n",
    "es $s'$\n",
    "\n",
    "Los $POMDPs$ son extensiones de los $MDPs$ en dónde no se conoce con certeza el\n",
    "estado actual, en su lugar el planificador debe construir y actualizar en tiempo de ejecución una distribución de probabilidad sobre el conjunto de estados $S$ representando ası́\n",
    "su creencia (o $belief$) sobre el estado actual. En un $POMDP$ , cada vez que se realiza una\n",
    "acción se desconoce el estado concreto que se alcanza, pero se obtiene una observación\n",
    "$z\\ ∈\\ Z$ que permite actualizar el $belief$, es por ello que se denominan $parcialmente\\ observables$. Nótese también que la función de recompensa no considera el estado alcanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. El problema del tigre\n",
    "Uno de los ejemplos más clásicos de $POMDPs$ es el problema del tigre que se\n",
    "ilustra en la figura 2.\n",
    "Imagina que te encuentras ante dos puertas, tras una de ellas hay un tigre hambriento,\n",
    "tras la otra un camino para escapar. Debes abrir una puerta, pero no sabes en qué puerta\n",
    "está el tigre. Como acciones puedes abrir una de las dos puertas o escuchar. Al escuchar\n",
    "puedes percibir que el tigre se encuentra a la izquierda o a la derecha. Pero hay que tener\n",
    "en cuenta que debido al estrés y los nervios, tus sentidos te pueden engañar y puedes\n",
    "escuchar al tigre en un lado cuando en realidad se encuentra al otro. Para colmo, el\n",
    "tigre se va impacientando y es capaz de romper la puerta al cabo de un tiempo. ¿Qué\n",
    "secuencia de acciones deberı́as realizar para escapar lo antes posible?\n",
    "\n",
    "El problema del tigre se puede formalizar como un $POMDP$ = $(S, A, T, R, Z, O)$ de\n",
    "la siguiente forma:\n",
    "* $S$ = {$tigre\\_izquierda, tigre\\_derecha$}\n",
    "* $A$ = {$escuchar, abrir\\_izquierda, abrir\\_derecha$}\n",
    "* $T (s, a, s')$ = 1 si s = s' para cualquier $a$ ∈ $A$\n",
    "* $T (s, a, s')$ = 0 si s $\\neq$ s' para cualquier $a$ ∈ $A$\n",
    "* $R(tigre\\_izquierda, abrir\\_izquierda)$ = −100\n",
    "* $R(tigre\\_derecha, abrir\\_derecha)$ = −100\n",
    "* $R(tigre\\_izquierda, abrir\\_derecha)$ = 10\n",
    "* $R(tigre\\_derecha, abrir\\_izquierda)$ = 10\n",
    "* $R(s, escuchar)$ = −1 para cualquier $s$ ∈ $S$\n",
    "* $Z$ = {$rugidos\\_a\\_la\\_izquierda, rugidos\\_a\\_la\\_derecha$}\n",
    "* $O(tigre\\_izquierda, escuchar, rugidos\\_izquierda)$ = 0,85\n",
    "* $O(tigre\\_izquierda, escuchar, rugidos\\_derecha)$ = 0,15\n",
    "* $O(tigre\\_derecha, escuchar, rugidos\\_derecha)$ = 0,85\n",
    "* $O(tigre\\_derecha, escuchar, rugidos\\_izquierda)$ = 0,15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
